# KAIST

This folder contains useful tools for benchmarking models against the KAIST character data set(though could be adapted to other datasets)

## Folder Structure

These subfolders should be present:

* `checkpoints`: set of checkpoint weights generated by YOLO to benchmark against
* `yolov3`: symbolically-linked module folder from [here](../yolov3/yolov3) containing functions for detection and evaluation
* `config`: (symbolically-linked) folder with config files for YOLO. At minimum, the class `.names` and YOLOv3 `.cfg` files should be here for the scripts to work
* `data`: contains the KAIST dataset (see the setup section to generate this)
* `output`: contains output benchmarks and graphs

## Pipeline

These steps assume a fresh clone from GitHub and the appropriate data folders (aside from `data`)have been set up. This pipeline should be re-executed every time you want to benchmark a completely new model

### Download the KAIST Data

Skip this step if `./data` is already sorted into the `images` and `labels` subfolders

1. `mkdir data` and `cd` into it
2. From there, run `get.py`. This should download the KAIST data and unzip it recursively while also removing image masks (not needed for YOLO)

### Cleanup and Extracting Letters

Change back into the KAIST directory and run `parse.py`. This moves all of the labeled scene images into its own folder, creates bounding boxes in Darknet `.txt` format for each image, and generates train/test lists using iterative stratification if you wish to build a model primarily using KAIST data.

The script also crops out characters from scene images, sorting them into subfolders in `data/obj/` by class. The class list from `config/chars.names` is used for this. **Remember to run this each time your classes change**

### Benchmark

After copying your YOLO weights to `checkpoints`, run
```
python3 benchmark.py <prefix> <epoch number>`
```

This will benchmark inferencing results for `checkpoints/<prefix>_ckpt_<epoch number>.pth`.

Inferencing is done on the images files  currentlyin `data/obj/`, which should only contain images of classes the model has been trained on. If not, re-run `parse.py`. Results are saved in `output/benchmark_<epoch>.csv`. Keep in mind that the script takes only the detected label/class with the highest confidence and saves it.

### Analyzing Results

Run `python3 histogram.py output/<benchmark>.csv`. This is only dependent on the benchmark spreadsheet, so analyses can be re-run on past benchmarks (even across different models).

A confusion matrix and two histogram files are generated in `output`. One histogram PDF groups results by the *actual* class (as determined by KAIST annotations), while the other groups results by the *predicted* class. The latter is what the sampling algorithm would see, as it does not know the ground truth. Green/red denote a hit/miss for the inferred class (or lack thereof), and accuracy and precision per class (actual or predicted) are calculated based the following:

* Accuracy = (true positives + true negatives) / population size
* Precision = true positives / (true positives + false positives)

Note that neither of these metrics account for false negatives (which should probably be worked in), but this isn't detrimental to the sampling/retraining process


### Sampling and Retraining

Sampling on a particular `benchmark_<ckpt num>.csv` file will generate config files for retraining in `output/configs-retrain`:

```
python3 sample.py output/benchmark_<ckpt>.csv
```

These training lists can then be augmented and used with the YOLOv3 `train.py` script directly. The testing list is generated in `config/test-new.txt`, which contains redirected paths to the original testing images. 

See `retrain.py` for how the whole pipeline interacts.


## Results

On the Google Drive, there are several sets of checkpoints, log file outputs, and benchmark results. Corresponding configurations exist in the [`yolov3`](../yolov3) folder, checked into GitHub. The checkpoints and log files are a *primary* copy of the data generated from training, while benchmarks can be regenerated from running the scripts in this folder with the corresponding class names.

Model Number | Config Folder | Description | Augmentation | Image Size | Batch Size
------------ | ------------- | ----------- | ------------ | ---------- | --------
1 | `config-char-orig` | Original baseline with ~0.92 mAP, using the 8 most frequent characters in the 74K data set and stratified random sampling | 8 filters, ran 5x each for 41x increase per training image | 416x416 | 64
2 | `config-char-30` | Model with 30 most frequent alphabetical characters, trained using undersampling. Near-zero mAP on test set, ~0.275 precision on KAIST data. Poor results likely due to small training set size (84 raw images per class) and low image size. | Same as (1) | 128x128 | 16
3 | `config-char12-origaug` | Curated set of 12 classes with high confusion rates based on (2), trained using undersampling (100 training images per class). Near-zero mAP on test set, ~0.56 precision on KAIST data. *Very* few object detections, leading to a small (<100 images) and/or inaccurate sampling pool.  | Same as (1) | 256x256 | 32
4 | `config-char12-origaug` (batch size=64, image size=416 now) | Train/test splits are the same as (3), only with new augmentation techniques. ~0.86 mAP on test set, ~0.86 precision on KAIST data. Model after 100 epochs used as baseline (converges after around epoch 50) | Composite transformation of one "major" transformation and 1-2 "minor" ones, applied randomly. 121x increase per training image. | 416x416 | 64
5 | `config` | Train/test splits for the same 12 classes are stratified but not undersampled, using augmentation to balance classes. ~0.888 mAP on KAIST data,  ~0.906 mAP on test data | Composite transforms, 15K images per class | 416x416 | 32

## Sampling Methods

These algorithms are meant to be scalable to any number of classes and any model, not just the KAIST data set. 

Before any sampling is done, we determine **Smax**, the maximum desired **number of retraining samples per class**. Assuming we have a list of `ClassResults` objects by *predicted* (not actual) class, this is done by first determining the number of (KAIST) samples we have above a user-defined **confidence threshold** (T>=0.5) for each class. The smallest number of samples we have above T for any class then becomes **Smax**.

### Threshold Sampling

We randomly sample a proportion (up to 1.00) of the samples above a threshold Tsample, equal to the median confidence of a particular class. 