# KAIST

This folder contains useful tools for benchmarking models against the KAIST character data set(though could be adapted to other datasets)

## Folder Structure

These subfolders should be present:

* `checkpoints`: set of checkpoint weights generated by YOLO to benchmark against
* `yolov3`: symbolically-linked module folder from [here](../yolov3/yolov3) containing functions for detection and evaluation
* `config`: (symbolically-linked) folder with config files for YOLO. At minimum, the class `.names` and YOLOv3 `.cfg` files should be here for the scripts to work
* `data`: contains the KAIST dataset (see the setup section to generate this)
* `output`: contains output benchmarks and graphs

## Pipeline

These steps assume a fresh clone from GitHub and the appropriate data folders (aside from `data`)have been set up. This pipeline should be re-executed every time you want to benchmark a completely new model

### Download the KAIST Data

Skip this step if `./data` is already sorted into the `images` and `labels` subfolders

1. `mkdir data` and `cd` into it
2. From there, run `get.py`. This should download the KAIST data and unzip it recursively while also removing image masks (not needed for YOLO)

### Cleanup and Extracting Letters

Change back into the KAIST directory and run `parse.py`. This moves all of the labeled scene images into its own folder, creates bounding boxes in Darknet `.txt` format for each image, and generates train/test lists using iterative stratification if you wish to build a model usng only KAIST data.

The script also crops out characters from scene images, sorting them into subfolders in `data/obj/` by class. The class list from `config/chars.names` is used for this. **Remember to run this each time your classes change**

### Benchmark

After copying your YOLO weights to `checkpoints`, run `python3 benchmark.py <epoch number>`, which will benchmark inferencing results for `checkpoints/yolov3_ckpt_<epoch number>.pth`.

Inferencing is done on the images files  currentlyin `data/obj/`, which should only contain images of classes the model has been trained on. If not, re-run `parse.py`. Results are saved in `output/benchmark_<epoch>.csv`. Keep in mind that the script takes only the detected label/class with the highest confidence and saves it.

### Analyzing Results

Run `python3 histogram.py output/<benchmark>.csv`. This is only dependent on the benchmark spreadsheet, so analyses can be re-run on past benchmarks (even across different models).

A confusion matrix and two histogram files are generated in `output`. One histogram PDF groups results by the *actual* class (as determined by KAIST annotations), while the other groups results by the *predicted* class. The latter is what the sampling algorithm would see, as it does not know the ground truth. Green/red denote a hit/miss for the inferred class (or lack thereof), and accuracy and precision per class (actual or predicted) are calculated based the following:

* Accuracy = (true positives + true negatives) / population size
* Precision = true positives / (true positives + false positives)

Note that neither of these metrics account for false negatives (which should probably be worked in), but this isn't detrimental to the sampling/retraining process


### Sampling and Retraining

To be completed...