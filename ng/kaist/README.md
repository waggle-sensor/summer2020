# KAIST

This folder contains useful tools for benchmarking models against the KAIST character data set(though could be adapted to other datasets)

## Folder Structure

These subfolders should be present:

* `checkpoints`: set of checkpoint weights generated by YOLO to benchmark against
* `yolov3`: symbolically-linked module folder from [here](../yolov3/yolov3) containing functions for detection and evaluation
* `config`: (symbolically-linked) folder with config files for YOLO. At minimum, the class `.names` and YOLOv3 `.cfg` files should be here for the scripts to work
* `data`: contains the KAIST dataset (see the setup section to generate this)
* `output`: contains output benchmarks and graphs

## Pipeline

These steps assume a fresh clone from GitHub and the appropriate data folders (aside from `data`)have been set up. This pipeline should be re-executed every time you want to benchmark a completely new model

### Download the KAIST Data

Skip this step if `./data` is already sorted into the `images` and `labels` subfolders

1. `mkdir data` and `cd` into it
2. From there, run `get.py`. This should download the KAIST data and unzip it recursively while also removing image masks (not needed for YOLO)

### Cleanup and Extracting Letters

Change back into the KAIST directory and run `parse.py`. This moves all of the labeled scene images into its own folder, creates bounding boxes in Darknet `.txt` format for each image, and generates train/test lists using iterative stratification if you wish to build a model primarily using KAIST data.

The script also crops out characters from scene images, sorting them into subfolders in `data/obj/` by class. The class list from `config/chars.names` is used for this. **Remember to run this each time your classes change**

### Benchmark

After copying your YOLO weights to `checkpoints`, run `python3 benchmark.py <epoch number>`, which will benchmark inferencing results for `checkpoints/yolov3_ckpt_<epoch number>.pth`.

Inferencing is done on the images files  currentlyin `data/obj/`, which should only contain images of classes the model has been trained on. If not, re-run `parse.py`. Results are saved in `output/benchmark_<epoch>.csv`. Keep in mind that the script takes only the detected label/class with the highest confidence and saves it.

### Analyzing Results

Run `python3 histogram.py output/<benchmark>.csv`. This is only dependent on the benchmark spreadsheet, so analyses can be re-run on past benchmarks (even across different models).

A confusion matrix and two histogram files are generated in `output`. One histogram PDF groups results by the *actual* class (as determined by KAIST annotations), while the other groups results by the *predicted* class. The latter is what the sampling algorithm would see, as it does not know the ground truth. Green/red denote a hit/miss for the inferred class (or lack thereof), and accuracy and precision per class (actual or predicted) are calculated based the following:

* Accuracy = (true positives + true negatives) / population size
* Precision = true positives / (true positives + false positives)

Note that neither of these metrics account for false negatives (which should probably be worked in), but this isn't detrimental to the sampling/retraining process


### Sampling and Retraining

To be completed...

## Results

On the Google Drive, there are several sets of checkpoints, log file outputs, and benchmark results. Corresponding configurations exist in the `[yolov3](../yolov3)` folder, checked into GitHub. The checkpoints and log files are a *primary* copy of the data generated from training, while benchmarks can be regenerated from running the scripts in this folder with the corresponding class names.

Model Number | Config Folder | Description | Augmentation | Image Size | Batch Size
------------ | ------------- | ----------- | ------------ | ------
1 | `config-char-orig` | Original baseline with ~0.92 mAP, using the 8 most frequent characters in the 74K data set and stratified random sampling | 8 filters, ran 5x each for 41x increase per training image | 416x416 | 64
2 | `config-char-30` | Model with 30 most frequent alphabetical characters, trained using undersampling. Near-zero mAP on test set, ~0.275 precision on KAIST data. Poor results likely due to small training set size (84 raw images per class) and low image size. | Same as (1) | 128x128 | 16
3 | `config` | Curated set of 12 classes with high confusion rates based on (2), trained using undersampling (100 training images per class). Near-zero mAP on test set, ~0.56 precision on KAIST data. *Very* few object detections, leading to a small (<100 images) and/or inaccurate sampling pool.  | Same as (1) | 256x256 | 32
4 | `config` (batch size=64, image size=416 now) | Train/test splits are the same as (3), only with new augmentation techniques. ~0.6 mAP on test set, ~0.86 precision on KAIST data. Model after 100 epochs used as baseline (converges after around epoch 50) | Composite transformation of one "major" transformation and 1-2 "minor" ones, applied randomly. 121x increase per training image. | 416x416 | 64