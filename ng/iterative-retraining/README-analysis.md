# Analysis Tool Usage

Aside from the [main module](./__main__.py) for starting the sampling and retraining pipeline, a [script for analyzing training results](./analyze.py) is included in this folder. The program supports the following features:

* Benchmarking against various test sets as a function of one or multiple averaged models and plotting the resultant training curves
* Tabulating performance metrics (precision, accuracy, recall, avg. confidence) for a particular benchmark
* Comparing metrics across various sampling methods using next-batch test methods
* Linear regression between two metrics to analyze correlation between sample features and performance improvements
* Generating confidence distributions of inference results for each sample batch
* Filtering sample batches for the specific images collected in that sample

To use the script, execute the following:

```
python3 analyze.py --config <retrain config file> [--benchmark [--avg | --roll <epoch span> --delta <epoch span>]] \
	(--prefix <sampling method> | --tabulate | --visualize <benchmark> | --view_benchmark <benchmark>) \
	[--filter_sample --compare_init --batch_test <test set size> --aggr_median] \
	[--metric <metric name> --metric2 <metric name>]
```

The only required argument is `--config` along with the path to the pipeline configuration file. However, for the script to produce a visual output, one of the additional flags in parentheses must be specified. The `--benchmark` flag may also be set to generate benchmark files if they have not yet been generated.

Many flags also have additional optional parameters for filtering out data. See below for a description of each flag and how flags may be used in conjunction.

## Benchmarking

There are two main types of benchmarking for accuracy, precision, and confidence, both of which are conducted when the `--benchmark` flag is specified:

* **Series benchmarking** continuously benchmarks checkpoint models at an interval specified by `--delta` (default of 4 epochs), for all models generated in the initial training or retraining process. This is done instantaneously for a single model by default. If `--avg` or `--roll_avg` are specified with an integer epoch span, a linearly-spaced or rolling average of checkpoint models will be taken instead. Files will be saved at the path `<output>/<prefix>-series[[-roll]-avg]/<test set>_<epoch>.csv`, where the test set name is one of the following:
  * `init`: test set generated from initial training
  * `sample`: all test sets generated from the sample images
  * `cur_iter`: test set of the current iteration when retraining, where this is a mix of 75% samples from the current iteration and 25% seen test set images
  * `all_iter`: a union of all images in iteration test sets
  * `all`: a union of all of the above images
* **Batch split benchmarking** finds the epochs at which training for a particular batch of images (either samples or the initial training set) ends and benchmarks the performance of the models thus far against a test set that the model has not yet seen. By default, this uses all images in the next batch of images and benchmarks using a linearly-sapced model average. The `--roll_avg` flag may be set to use a rolling average instead. Note that these benchmarks are generated as a byproduct of the sampling pipeline. Output files have the format `<output>/<prefix><next batch number>_benchmark[_roll]_avg[_test]_1_<last epoch>.csv` (i.e. `sampling0_benchmark_avg_1_50.csv` will  benchmark the initial training models, and `sampling1_benchmark_avg_1_75.csv` will benchmark the models after the first retraining iteration).

**Additional Notes**

* If the `--batch_test <N>` flag is specified with a certain number of image batches, the last `N` batches sampled upon for iterative retraining will be set aside and used as a consistent test set across all epoch models. This will replace next-batch testing when benchmarking at batch splits (appending `_test` to the output file), and an additional test set will be added (as `batch_test`) in series benchmarking. The models generated during the last `N` iterations of training (where training is done with images in this test set) will also not be benchmarked. 
* By default, if the `--prefix <sampling method>` flag is not specified, all sampling methods will be benchmarked. This is parallelized across GPUs if `parallel` is true in the configuration file. If the `--prefix` flag is set, benchmarking will only be done on one sampling method.
* Aside from filename differences, all benchmark files have contents that follow the [benchmarks generated from the sampling pipeline](./README.md#training-output).
* Benchmarking is the first step done in the script, so `--benchmark` may be used alongside one of the visualization options.

## Training Curves

Plots of a sampling method's performance over time can be viewed by specifying its name with `--prefix <name>`. A precision-epoch curve will be generated by default, with lines representing each of the test sets created via series benchmarking. Click on the lines in the legend to enable/disable them.

Other metrics besides precision can be viewed by specifying one of the following metric names with the `--metric` flag: 
* `acc`
* `recall`
* `conf` (for mean average confidence, aggregated across class)
* `conf_std` (for average standard deviation in confidence values, aggregated across class)
* `detect_conf_std` (for the average standard deviation across each detection when averaging them together)

A line for the batch test set can be added by specifying the `--batch_test <N>` flag.

## Sampling Method Metric Tables

Tables of the available metrics (`prec`, `acc`, `recall`, `conf`, `conf_std`, `detect_conf_std`, and `epochs_trained`) can be generated for a particular sampling method (with various metrics shown) or for a particular metric (with various sampling methods shown) with the `--tabulate` flag. To see the former, specify the sampling method with `--prefix`, or see the latter with the `--metric` flag. If neither is specified, `--tabulate` will generate a table of average precisions across all sampling methods.

In either case, metrics or sampling methods are displayed as rows, with the batch split benchmaarks as columns, indexed by iteration number. The default option is to view next-batch benchmarking results, though you can view batch test set benchmarking results with the `--batch_test <N>` flag.

**Additional Options**
* Add the `--compare` flag to show increases/decreases in a metric (precision, accuracy, etc.) against the initial baseline (as shown by the `init` prefix)
* Add the `--filter_sample` flag to filter results to account only for images that are later used for that iteration's sample set

## Comparing Metrics

Using batch split benchmarks, metrics of the samples in each sampling method can be correlated against the performance of the resultant models after retraining at each batch. By specifying metric names with `--metric` and `--metric2`, a linear regression will be run by averaging `--metric` of the samples for a particular sampling method and `--metric2` of the resultant models. That is, a point will be generated for each sampling method that is present, with average `metric` (across the batches of that method) on the x-axis and average `metric2` on the y-axis. Statistics of that regression and the points used to generate it will also be printed.

**Additional Options**
* Add the `--aggr_median` flag to aggregate using a median instead an average. This is useful when there may be confounding factors skewing the batch splits, such as the number of epochs trained
* The `--compare` flag may be used to show increases/decreases against the `init` baseline
* Specify the `--batch_test <N>` flag to run a regression using the batch test benchmarks instead of the next-batch benchmarks. In this case, the `--compare` flag will only shift the results by a constant amount, as the test set is constant across batch splits. The last `N` batch splits will also not be used in the regression.

## Benchmark Metric Table

View the individual and averaged metrics on a per-class basis on a benchmark with the `--view_benchmark <benchmark file>` option. A table of results will also be generated `<benchmark filename>_stats.csv`. All other flags are ignored.

## Benchmark Confidence Histogram

Use `--visualize_conf <benchmark file>` to view confidence histogram distributions of hits (accurate label predictions) and misses (inaccurate predictions) for a particular benchmark. Three histograms (hits, misses, and combined) will be shown, and a PDF of the confidence distribution per class will be generated at `<benchmark filename>_viz.pdf`. A confusion matrix will also be generated at `<benchmark filename>_conf.csv` along with a CSV file containing the rolling precision of the distribution (at 0.05 confidence intervals) at `<benchmark filename>_prec.csv`.

Use the `--filter_sample` flag to filter by images in the sample if (and only if) viewing a benchmark file at a batch split epoch. When using this option, `--prefix` must be specified to fetch the correct sample files.
